{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessiomongoli/Sentiment_Lexicon/blob/main/Data_induction/PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from spacy.symbols import neg\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import RegexpTokenizer\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gzip\n",
        "import json\n",
        "import time\n",
        "import spacy\n",
        "import re"
      ],
      "metadata": {
        "id": "4RgBh9BpNR-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4107f2-3901-42c8-de07-f5b56b14c134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kw7JqRxuf4T"
      },
      "outputs": [],
      "source": [
        "# Read and load the catagories\n",
        "def parseAmazon(path):\n",
        "    g = gzip.open(path, 'rb')\n",
        "    for line in g:\n",
        "      yield json.loads(line)    \n",
        "\n",
        "def preprocessing(path, negation_type, category): \n",
        "    start = time.time()\n",
        "    reviews = []\n",
        "    labels  = []           \n",
        "\n",
        "    for index, review in enumerate(parseAmazon(path)):\n",
        "          if review[\"overall\"] != 3.0:\n",
        "              if negation_type == 'normal':\n",
        "                  neg_review = normal_negation(review[\"reviewText\"])\n",
        "              elif negation_type == 'all_words':\n",
        "                  neg_review = all_words_negation(review[\"reviewText\"])\n",
        "              else :\n",
        "                neg_review = no_negation(review[\"reviewText\"])\n",
        "              if review[\"overall\"] < 3.0:    \n",
        "                  label = -1\n",
        "              else:\n",
        "                  label = +1\n",
        "              reviews.append(neg_review)\n",
        "              labels.append(label)\n",
        "\n",
        "    y = np.array(labels)\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(reviews)\n",
        "    vocabulary = vectorizer.vocabulary_\n",
        "    frequencies = X.sum(axis=0)\n",
        "    frequencies = np.asarray(frequencies)[0]\n",
        "    end = time.time()\n",
        "    with open('/content/drive/MyDrive/Github/Colab Notebooks/project/Results/time.txt', 'a') as f:\n",
        "      f.writelines('\\n'+category+'_'+negation_type+' Preprocessing phase: '+str(end-start)+' seconds')\n",
        "      f.close()\n",
        "    return X, y, frequencies, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_experiment_price(df, negation_type): # experiment\n",
        "    reviews = []\n",
        "    labels  = []           \n",
        "\n",
        "    for index, review in df.iterrows():\n",
        "        if negation_type == 'normal':\n",
        "          neg_review = normal_negation(review[\"reviewText\"])\n",
        "        elif negation_type == 'all_words':\n",
        "          neg_review = all_words_negation(review[\"reviewText\"])\n",
        "        else:\n",
        "          neg_review = no_negation(review[\"reviewText\"])\n",
        "        if review[\"label\"] < 3.0:\n",
        "          label = -1\n",
        "        else:\n",
        "          label = +1\n",
        "        reviews.append(neg_review)\n",
        "        labels.append(label)\n",
        "\n",
        "    y = np.array(labels)\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(reviews)\n",
        "    vocabulary = vectorizer.vocabulary_\n",
        "    frequencies = X.sum(axis=0)\n",
        "    frequencies = np.asarray(frequencies)[0]\n",
        "    return X, y, frequencies, vocabulary"
      ],
      "metadata": {
        "id": "gHBx_Qyw_A_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decontracted(review):\n",
        "    review = re.sub(r\"isn\\'t\", \"is not\", review)\n",
        "    review = re.sub(r\"won\\'t\", \"will not\", review)\n",
        "    review = re.sub(r\"can\\'t\", \"can not\", review)\n",
        "    review = re.sub(r\"wasn\\'t\", \"was not\", review)\n",
        "    review = re.sub(r\"weren\\'t\", \"were not\", review)\n",
        "    review = re.sub(r\"aren\\'t\", \"are not\", review)\n",
        "    review = re.sub(r\"couldn\\'t\", \"could not\", review)\n",
        "    review = re.sub(r\"don\\'t\", \"do not\", review)\n",
        "    review = re.sub(r\"didn\\'t\", \"did not\", review)\n",
        "    review = re.sub(r\"doesn\\'t\", \"does not\", review)\n",
        "    review = re.sub(r\"haven\\'t\", \"have not\", review)\n",
        "    review = re.sub(r\"hadn\\'t\", \"had not\", review)\n",
        "    return review"
      ],
      "metadata": {
        "id": "LUxLUKTTd5Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPl_Wo2QhrSy"
      },
      "outputs": [],
      "source": [
        "def normal_negation(review):    \n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    review = decontracted(review)    \n",
        "    tokens = tokenizer.tokenize(review)\n",
        "    clean_review = ''\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token == 'not' and i != len(tokens) - 1:\n",
        "            tokens[i + 1] = 'NEG_' + tokens[i + 1]\n",
        "        else:\n",
        "            if token.startswith('NEG_'):\n",
        "                clean_review = clean_review + ' ' + token\n",
        "            else:\n",
        "                clean_review = clean_review + ' ' + token.lower()\n",
        "    return clean_review.strip()\n",
        "\n",
        "\n",
        "def no_negation(review):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    review = decontracted(review)    \n",
        "    result = list()\n",
        "    for sent in sent_tokenize(review):\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "        result.extend(tokens)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "def all_words_negation(review):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    review = decontracted(review)    \n",
        "    result = list()\n",
        "    for sent in sent_tokenize(review):\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token == 'not' and i != len(tokens) - 1:\n",
        "                for j in range(i + 1, len(tokens)):\n",
        "                    tokens[j] = 'NEG_' + tokens[j]\n",
        "                break\n",
        "        result.extend(tokens)\n",
        "    return ' '.join(result)"
      ]
    }
  ]
}