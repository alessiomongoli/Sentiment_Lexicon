{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "negation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessiomongoli/Sentiment_Lexicon/blob/main/negation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LPl_Wo2QhrSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e1b168-7163-42bf-f242-ef3f4ccf80cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from spacy.symbols import neg\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "NEGATION_TOKENS = {\"not\", \"nor\", \"never\"}  # example\n",
        "\n",
        "\n",
        "def find_negations(review, tokenizer):\n",
        "    \"\"\"\n",
        "    sentence negation processing : \n",
        "    the word just after a negation ('not') is negated\n",
        "    ex : \n",
        "    - I did not like the movie.\n",
        "    >> I did NEGATEDWlike the movie.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(review)\n",
        "    clean_review = ''\n",
        "    for i, t in enumerate(tokens):\n",
        "        if t == 'not' and i != len(tokens) - 1:\n",
        "            tokens[i + 1] = 'NEGATEDW' + tokens[i + 1]\n",
        "        else:\n",
        "            clean_review = clean_review + ' ' + t.lower()\n",
        "    return clean_review.strip()\n",
        "\n",
        "\n",
        "def whole_sentence_negation(review, tokenizer, negation_tokens=NEGATION_TOKENS):\n",
        "    \"\"\"\n",
        "    sentence negation processing : \n",
        "    every word inbetween a negation token and the end of the sentence will be considered negative\n",
        "    ex : (with negation_tokens containing {'not', 'never', 'nor'})\n",
        "    - I did not like the movie.\n",
        "    >> I did NEGATEDWlike NEGATEDWthe NEGATEDWmovie.\n",
        "    \"\"\"\n",
        "    negation_prefix = 'NEGATEDW'\n",
        "    result = list()\n",
        "    for sent in sent_tokenize(review):\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "        for i, t in enumerate(tokens):\n",
        "            if t in negation_tokens and i != len(tokens) - 1:\n",
        "                for j in range(i + 1, len(tokens)):\n",
        "                    tokens[j] = negation_prefix + tokens[j]\n",
        "                break\n",
        "        result.extend(tokens)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "def find_complex_negations(review, parser):\n",
        "    \"\"\"\n",
        "        sentence negation processing :\n",
        "        we negate the words which are referenced through a \"neg\" dependency edge of Spacy dependency parser.\n",
        "        - I did not really like the movie.\n",
        "        >> I did really NEGATEDWlike the movie.\n",
        "        \"\"\"\n",
        "    negation_prefix = 'NEGATEDW'\n",
        "    complete_tokens = []\n",
        "    review = review.lower()\n",
        "    for sent in sent_tokenize(review):\n",
        "        doc = parser(sent)\n",
        "        final_tokens = [w.text for w in doc]\n",
        "        idx_to_delete = []\n",
        "        for i, token in enumerate(doc):\n",
        "            if token.dep == neg:\n",
        "                idx_to_delete.append(i)\n",
        "                for j, final_token in enumerate(final_tokens):\n",
        "                    if final_token == str(token.head):\n",
        "                        final_tokens[j] = negation_prefix + final_token\n",
        "                        break\n",
        "        for index, i in enumerate(idx_to_delete):\n",
        "            del final_tokens[i - index]\n",
        "        complete_tokens = complete_tokens + final_tokens\n",
        "    return ' '.join(complete_tokens).strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('hello')"
      ],
      "metadata": {
        "id": "cJ6shOBKA03t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7569cdbc-5813-4d8c-d0ac-7b82c12dda01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xIxeDuSIkYoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}