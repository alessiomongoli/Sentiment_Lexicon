{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessiomongoli/Sentiment_Lexicon/blob/main/FileGenerale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alessiomongoli/Sentiment_Lexicon.git"
      ],
      "metadata": {
        "id": "umPRUW_QELK_",
        "outputId": "adb4cf48-81a3-4c87-a9a6-e02ce15d9e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sentiment_Lexicon'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 128 (delta 63), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (128/128), 45.67 KiB | 11.42 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gulEtb-M8IO",
        "outputId": "44622d55-dbd3-489c-90a7-efac35454be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.5.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.11.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.12.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "pip install import-ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G50b88hLKvGi"
      },
      "outputs": [],
      "source": [
        "import import_ipynb\n",
        "import sys\n",
        "sys.path.append('/content/Sentiment_Lexicon/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk5Idf5fK2q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2929bbcf-4eca-4ddf-81cf-8eb717459144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Sentiment_Lexicon\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Sentiment_Lexicon/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food.json.gz\n"
      ],
      "metadata": {
        "id": "uyH5n2n4FGv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1mpL3_URUIDlY0I5MiaCi_tWXqGdA2-6k"
      ],
      "metadata": {
        "id": "zdpVWwRVVAoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1PQ51btvMqsmhNQEUhxaOKlfaL2K-pSV2"
      ],
      "metadata": {
        "id": "Ey8_VbwbLzLy",
        "outputId": "d3b9ee66-17c6-423c-df8f-c932dd803d39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1PQ51btvMqsmhNQEUhxaOKlfaL2K-pSV2\n",
            "To: /content/IMDB-Dataset_preprocessed.csv\n",
            "100% 64.7M/64.7M [00:00<00:00, 279MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HePg75yWO-6c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "import string\n",
        "\n",
        "\n",
        "from Data_induction.PreProcessing import preprocessing\n",
        "from Data_induction.TrainSVM import LinearCoefficentsSVM\n",
        "from glove.Glove import load_filter_glove_word\n",
        "from Data_induction.Seed_Data import SeedDataset\n",
        "from Neural_model.EarlyStopping import EarlyStopping\n",
        "from Neural_model.Neural import RegressionModel\n",
        "from Neural_model.Train_predict_NN import train, predict\n",
        "from Utils.DataframeCreation import creation_dataframe\n",
        "from Test.Experiment1 import experiment1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7GIERvxhWRH"
      },
      "outputs": [],
      "source": [
        "CATEGORY=\"reviews_Grocery_and_Gourmet_Food\" #\"reviews_Apps_for_Android\" #reviews_Apps_for_Android.json.gz or reviews_Grocery_and_Gourmet_Food.json.gz or reviews_Movies_and_TV.json.gz\n",
        "SAVE_DATAFRAME_FILE= False\n",
        "CHECKPOINT_PATH = \"/content/Sentiment_Lexicon/checkpoint/checkpoint.pt\"\n",
        "NEGATION_TYPE='all_words' # 'normal' or 'all_words' or 'no_negation'\n",
        "NEGATION_MEAN=True\n",
        "REPOSITORY = '/content/Sentiment_Lexicon/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0TAvcsvWFzY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\" Preprocessing function:\n",
        "1 Apre il file dove ci sono Recensioni e score.\n",
        "2 Filtra le recensioni togliendo quelle con 3 stelle\n",
        "3Usa la funzione find negation per aggiungere dei tag nelle recensioni dove trova vocaboli negativi {not never nor}\n",
        "4)Crea due liste:recensioni, label (1 se >3 stelle, -1< 3 stelle)\n",
        "5) Genera il document term Matrix \n",
        "6) Ritorna:\n",
        "X Bag of word\n",
        "Y array di label 1 o 0\n",
        "Frequenze lista con le occurences\n",
        "Vocabulary \n",
        "\"\"\"\n",
        "\n",
        "X, y, frequencies, vocabulary = preprocessing(REPOSITORY+CATEGORY+\".json.gz\", NEGATION_TYPE, CATEGORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMD0uTLPZTxN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fa il training di SVM con le BOW e le label ed estrae i pesi che vengono considerate come polarità\n",
        "\"\"\"\n",
        "coefficents = LinearCoefficentsSVM(X, y, CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3mchi57jNeb"
      },
      "outputs": [],
      "source": [
        "Data=creation_dataframe(vocabulary,coefficents, frequencies)\n",
        "if SAVE_DATAFRAME_FILE==True:\n",
        "  Data.to_csv('/content/Sentiment_Lexicon/Data/'+CATEGORY+'_'+NEGATION_TYPE+'.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WGw8MXIIqTm"
      },
      "outputs": [],
      "source": [
        "if SAVE_DATAFRAME_FILE==True:\n",
        "  Data = pd.read_csv(REPOSITORY+'Data/'+CATEGORY+'_'+NEGATION_TYPE+'.csv')\n",
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqvzCOcNllvz"
      },
      "outputs": [],
      "source": [
        "Data = Data[Data['Frequence']>=500]\n",
        "Data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ5eUoetl9Me"
      },
      "outputs": [],
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Negation_MEAN is true we consider also the Neg_word  adjust polarity of the word without Neg:\n",
        "# New Polarity [word]=(Polarity[word]+ Polarity[NEG_word])/2"
      ],
      "metadata": {
        "id": "yhRnCOK8kiCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if NEGATION_MEAN:\n",
        "  negated={}\n",
        "  prefix=len('neg_')\n",
        "  for i in range(len(Data)):\n",
        "    if str(Data['Token'][i]).startswith('neg_')==True:\n",
        "      negated[Data['Token'][i][prefix:]]=Data['Polarity'][i]\n",
        "  for i in range(len(Data)):\n",
        "    if Data['Token'][i]in negated.keys():\n",
        "      Data['Polarity'][i]=(negated[Data['Token'][i]]+Data['Polarity'][i]/2)"
      ],
      "metadata": {
        "id": "M6EiAbNcD3EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "KPvBaSp4iGep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKKjP-ckl-TQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#glove_embeddings_filtered= parole comuni tra glove e nostri tokens quindi automatimaticamente cancella le negation\n",
        "glove_embeddings_filtered, glove_wo_pol= load_filter_glove_word(REPOSITORY+'glove.840B.300d.pkl',list(Data['Token']), CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1n6F8OPnW7u"
      },
      "outputs": [],
      "source": [
        "len(glove_embeddings_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAz7Z9qkncWV"
      },
      "outputs": [],
      "source": [
        "Data=Data[Data['Token'].isin(glove_embeddings_filtered.keys())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om5YgBMbnWzt"
      },
      "outputs": [],
      "source": [
        "Data.reset_index(drop=True, inplace=True)\n",
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ACr-uJUnWxW"
      },
      "outputs": [],
      "source": [
        "Data['Embedding']=Data['Token'].apply(lambda x: glove_embeddings_filtered[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW3lbdGqnWq7"
      },
      "outputs": [],
      "source": [
        "Data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "OZhgVjh4z-Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZg0OArOnWkV"
      },
      "outputs": [],
      "source": [
        "Dataset=SeedDataset(list(Data['Token']),glove_embeddings_filtered, pol=list(Data['Polarity']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX3uVGVsnWgN"
      },
      "outputs": [],
      "source": [
        "trained_model = train(Dataset, CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjQeqav4nWcu"
      },
      "outputs": [],
      "source": [
        "all_vocabulary= Dataset.get_result()\n",
        "non_seed_data={w:0 for w in glove_wo_pol.keys()}\n",
        "non_seed_dataset = SeedDataset(list(non_seed_data.keys()),glove_wo_pol,split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PolA0aQFnWYH"
      },
      "outputs": [],
      "source": [
        "results = predict(trained_model, non_seed_dataset, CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_vocabulary.update(results)"
      ],
      "metadata": {
        "id": "t-BzAfdP6ZH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7tbdwDKnWNZ"
      },
      "outputs": [],
      "source": [
        "if NEGATION_MEAN:\n",
        "  df_IMDB = pd.read_csv(REPOSITORY+'IMDB-Dataset_preprocessed.csv')\n",
        "  accuracy_IMDB=experiment1(df_IMDB, complete_result, CATEGORY+'_'+NEGATION_TYPE+'_with_neg', 'IMDB')\n",
        "else:\n",
        "  df_IMDB = pd.read_csv(REPOSITORY+'IMDB-Dataset_preprocessed.csv')\n",
        "  accuracy_IMDB=experiment1(df_IMDB, complete_result, CATEGORY+'_'+NEGATION_TYPE, 'IMDB')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_IMDB"
      ],
      "metadata": {
        "id": "buo-lkImETkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NEGATION_MEAN:\n",
        "  df_tweets = pd.read_csv('/content/Sentiment_Lexicon/Data/Tweets_preprocessed.csv')\n",
        "  accuracy_tweet=experiment1(df_tweets, complete_result, CATEGORY+'_'+NEGATION_TYPE+'_with_neg', 'Tweets')\n",
        "else:\n",
        "  df_tweets = pd.read_csv('/content/Sentiment_Lexicon/Data/Tweets_preprocessed.csv')\n",
        "  accuracy_tweet=experiment1(df_tweets, complete_result, CATEGORY+'_'+NEGATION_TYPE, 'Tweets')\n",
        "accuracy_tweet"
      ],
      "metadata": {
        "id": "7UFYqmGOTVbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "FileGenerale.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}