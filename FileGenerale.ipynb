{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessiomongoli/Sentiment_Lexicon/blob/main/FileGenerale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alessiomongoli/Sentiment_Lexicon.git"
      ],
      "metadata": {
        "id": "umPRUW_QELK_",
        "outputId": "ecd5892f-8551-4bcc-9018-2b2d42443896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sentiment_Lexicon'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (118/118), done.\u001b[K\n",
            "remote: Total 125 (delta 61), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (125/125), 44.41 KiB | 7.40 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gulEtb-M8IO",
        "outputId": "87c80fb2-acdc-4abd-d6b5-6a38c00c44b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.15.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.11.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.8.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4\n"
          ]
        }
      ],
      "source": [
        "pip install import-ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G50b88hLKvGi"
      },
      "outputs": [],
      "source": [
        "import import_ipynb\n",
        "import sys\n",
        "sys.path.append('/content/Sentiment_Lexicon/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vk5Idf5fK2q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf50829-6416-4c7a-fb11-bc5d16b0d54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Sentiment_Lexicon\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Sentiment_Lexicon/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food.json.gz"
      ],
      "metadata": {
        "id": "uyH5n2n4FGv6",
        "outputId": "7ed8128f-6118-4877-dc98-7eca73971ade",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-15 18:19:40--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231127100 (220M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Grocery_and_Gourmet_Food.json.gz’\n",
            "\n",
            "reviews_Grocery_and 100%[===================>] 220.42M  26.3MB/s    in 10s     \n",
            "\n",
            "2022-07-15 18:19:51 (21.1 MB/s) - ‘reviews_Grocery_and_Gourmet_Food.json.gz’ saved [231127100/231127100]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HePg75yWO-6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d28679e-2bf1-401b-e099-fd877b758407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Data_induction/PreProcessing.ipynb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Data_induction/TrainSVM.ipynb\n",
            "importing Jupyter notebook from /content/Sentiment_Lexicon/glove/Glove.ipynb\n",
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Data_induction/Seed_Data.ipynb\n",
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Neural_model/EarlyStopping.ipynb\n",
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Neural_model/Neural.ipynb\n",
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Neural_model/Train_predict_NN.ipynb\n",
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Utils/DataframeCreation.ipynb\n",
            "importing Jupyter notebook from /content/Sentiment_Lexicon/Test/Experiment1.ipynb\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "import string\n",
        "\n",
        "\n",
        "from Data_induction.PreProcessing import preprocessing\n",
        "from Data_induction.TrainSVM import LinearCoefficentsSVM\n",
        "from glove.Glove import load_filter_glove_word\n",
        "from Data_induction.Seed_Data import SeedDataset\n",
        "from Neural_model.EarlyStopping import EarlyStopping\n",
        "from Neural_model.Neural import RegressionModel\n",
        "from Neural_model.Train_predict_NN import train, predict\n",
        "from Utils.DataframeCreation import creation_dataframe\n",
        "from Test.Experiment1 import experiment1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7GIERvxhWRH"
      },
      "outputs": [],
      "source": [
        "CATEGORY=\"reviews_Grocery_and_Gourmet_Food\" #\"reviews_Apps_for_Android\" #reviews_Apps_for_Android.json.gz or reviews_Grocery_and_Gourmet_Food.json.gz or reviews_Movies_and_TV.json.gz\n",
        "SAVE_DATAFRAME_FILE= False\n",
        "CHECKPOINT_PATH = \"/content/Sentiment_Lexicon/checkpoint/checkpoint.pt\"\n",
        "NEGATION_TYPE='all_words' # 'normal' or 'all_words' or 'no_negation'\n",
        "NEGATION_MEAN=True\n",
        "REPOSITORY = '/content/drive/MyDrive/Github/Colab Notebooks/project/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0TAvcsvWFzY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\" Preprocessing function:\n",
        "1 Apre il file dove ci sono Recensioni e score.\n",
        "2 Filtra le recensioni togliendo quelle con 3 stelle\n",
        "3Usa la funzione find negation per aggiungere dei tag nelle recensioni dove trova vocaboli negativi {not never nor}\n",
        "4)Crea due liste:recensioni, label (1 se >3 stelle, -1< 3 stelle)\n",
        "5) Genera il document term Matrix \n",
        "6) Ritorna:\n",
        "X Bag of word\n",
        "Y array di label 1 o 0\n",
        "Frequenze lista con le occurences\n",
        "Vocabulary \n",
        "\"\"\"\n",
        "\n",
        "X, y, frequencies, vocabulary = preprocessing(REPOSITORY+CATEGORY+\".json.gz\", NEGATION_TYPE, CATEGORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMD0uTLPZTxN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fa il training di SVM con le BOW e le label ed estrae i pesi che vengono considerate come polarità\n",
        "\"\"\"\n",
        "coefficents = LinearCoefficentsSVM(X, y, CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3mchi57jNeb"
      },
      "outputs": [],
      "source": [
        "Data=creation_dataframe(vocabulary,coefficents, frequencies)\n",
        "if SAVE_DATAFRAME_FILE==True:\n",
        "  Data.to_csv('/content/drive/MyDrive/Github/Colab Notebooks/project/Data/'+CATEGORY+'_'+NEGATION_TYPE+'.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WGw8MXIIqTm"
      },
      "outputs": [],
      "source": [
        "Data = pd.read_csv(REPOSITORY+'Data/'+CATEGORY+'_'+NEGATION_TYPE+'.csv')\n",
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqvzCOcNllvz"
      },
      "outputs": [],
      "source": [
        "Data = Data[Data['Frequence']>=500]\n",
        "Data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ5eUoetl9Me"
      },
      "outputs": [],
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if NEGATION_MEAN:\n",
        "  negated={}\n",
        "  prefix=len('neg_')\n",
        "  for i in range(len(Data)):\n",
        "    if str(Data['Token'][i]).startswith('neg_')==True:\n",
        "      negated[Data['Token'][i][prefix:]]=Data['Polarity'][i]\n",
        "  for i in range(len(Data)):\n",
        "    if Data['Token'][i]in negated.keys():\n",
        "      Data['Polarity'][i]=(negated[Data['Token'][i]]+Data['Polarity'][i]/2)"
      ],
      "metadata": {
        "id": "M6EiAbNcD3EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "KPvBaSp4iGep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKKjP-ckl-TQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#glove_embeddings_filtered= parole comuni tra glove e nostri tokens quindi automatimaticamente cancella le negation\n",
        "glove_embeddings_filtered, glove_wo_pol= load_filter_glove_word(REPOSITORY+'glove/glove.840B.300d.pkl',list(Data['Token']), CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1n6F8OPnW7u"
      },
      "outputs": [],
      "source": [
        "len(glove_embeddings_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAz7Z9qkncWV"
      },
      "outputs": [],
      "source": [
        "Data=Data[Data['Token'].isin(glove_embeddings_filtered.keys())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om5YgBMbnWzt"
      },
      "outputs": [],
      "source": [
        "Data.reset_index(drop=True, inplace=True)\n",
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ACr-uJUnWxW"
      },
      "outputs": [],
      "source": [
        "Data['Embedding']=Data['Token'].apply(lambda x: glove_embeddings_filtered[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW3lbdGqnWq7"
      },
      "outputs": [],
      "source": [
        "Data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "OZhgVjh4z-Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZg0OArOnWkV"
      },
      "outputs": [],
      "source": [
        "Dataset=SeedDataset(list(Data['Token']),glove_embeddings_filtered, pol=list(Data['Polarity']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX3uVGVsnWgN"
      },
      "outputs": [],
      "source": [
        "trained_model = train(Dataset, CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjQeqav4nWcu"
      },
      "outputs": [],
      "source": [
        "complete_result= Dataset.get_result()\n",
        "non_seed_data={w:0 for w in glove_wo_pol.keys()}\n",
        "non_seed_dataset = SeedDataset(list(non_seed_data.keys()),glove_wo_pol,split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PolA0aQFnWYH"
      },
      "outputs": [],
      "source": [
        "results = predict(trained_model, non_seed_dataset, CATEGORY, NEGATION_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complete_result.update(results)"
      ],
      "metadata": {
        "id": "t-BzAfdP6ZH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HYrapbuFN4NU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7tbdwDKnWNZ"
      },
      "outputs": [],
      "source": [
        "if NEGATION_MEAN:\n",
        "  df_IMDB = pd.read_csv(REPOSITORY+'IMDB-Dataset_preprocessed.csv')\n",
        "  accuracy=experiment1(df_IMDB, complete_result, CATEGORY+'_'+NEGATION_TYPE+'_with_neg', 'IBDM')\n",
        "else:\n",
        "  df_IMDB = pd.read_csv(REPOSITORY+'IMDB-Dataset_preprocessed.csv')\n",
        "  accuracy_IMDB=experiment1(df_IMDB, complete_result, CATEGORY+'_'+NEGATION_TYPE, 'IBDM')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_IMDB"
      ],
      "metadata": {
        "id": "buo-lkImETkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZsWxPZH-21m"
      },
      "outputs": [],
      "source": [
        "if NEGATION_MEAN:\n",
        "  df_PlayStore = pd.read_csv(REPOSITORY+'googleplaystore_user_reviews_preprocessed.csv')\n",
        "  accuracy_play=experiment1(df_PlayStore, complete_result, CATEGORY+'_'+NEGATION_TYPE+'_with_neg', 'GooglePlayStore')\n",
        "else:\n",
        "  df_PlayStore = pd.read_csv(REPOSITORY+'googleplaystore_user_reviews_preprocessed.csv')\n",
        "  accuracy_play=experiment1(df_PlayStore, complete_result, CATEGORY+'_'+NEGATION_TYPE, 'GooglePlayStore')\n",
        "accuracy_play"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if NEGATION_MEAN:\n",
        "  df_tripadvisor = pd.read_csv('/content/drive/MyDrive/Github/Colab Notebooks/project/tripadvisor_hotel_reviews_preprocessed.csv')\n",
        "  accuracy_trip=experiment1(df_tripadvisor, complete_result, CATEGORY+'_'+NEGATION_TYPE+'_with_neg', 'TripAdvisor')\n",
        "else:\n",
        "  df_tripadvisor = pd.read_csv('/content/drive/MyDrive/Github/Colab Notebooks/project/tripadvisor_hotel_reviews_preprocessed.csv')\n",
        "  accuracy_trip=experiment1(df_tripadvisor, complete_result, CATEGORY+'_'+NEGATION_TYPE, 'TripAdvisor')\n",
        "accuracy_trip"
      ],
      "metadata": {
        "id": "tLmaXvILk3Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NEGATION_MEAN:\n",
        "  df_tweets = pd.read_csv('/content/drive/MyDrive/Github/Colab Notebooks/project/Tweets_preprocessed.csv')\n",
        "  accuracy_tweet=experiment1(df_tweets, complete_result, CATEGORY+'_'+NEGATION_TYPE+'_with_neg', 'Tweets')\n",
        "else:\n",
        "  df_tweets = pd.read_csv('/content/drive/MyDrive/Github/Colab Notebooks/project/Tweets_preprocessed.csv')\n",
        "  accuracy_twet=experiment1(df_tweets, complete_result, CATEGORY+'_'+NEGATION_TYPE, 'Tweets')\n",
        "# valore prima 0.7131097825145135\n",
        "accuracy_tweet"
      ],
      "metadata": {
        "id": "qOSxNel9AhY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NEGATION_MEAN:\n",
        "  df_tweets_4600 = pd.read_csv('/content/drive/MyDrive/Github/Colab Notebooks/project/Tweets_preprocessed_4600.csv')\n",
        "  accuracy_tweet46=experiment1(df_tweets_4600, complete_result, CATEGORY+'_'+NEGATION_TYPE+'_with_neg', 'Tweets4600')\n",
        "else:\n",
        "  df_tweets_4600 = pd.read_csv('/content/drive/MyDrive/Github/Colab Notebooks/project/Tweets_preprocessed_4600.csv')\n",
        "  accuracy_tweet46=experiment1(df_tweets_4600, complete_result, CATEGORY+'_'+NEGATION_TYPE, 'Tweets4600')\n",
        "accuracy_tweet46="
      ],
      "metadata": {
        "id": "7UFYqmGOTVbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "FileGenerale.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}